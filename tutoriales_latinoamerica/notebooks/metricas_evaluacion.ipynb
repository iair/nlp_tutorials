{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métricas de evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "832b7831d44243d9945e89d5cda95b02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "widgets.IntSlider()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similitud de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación con múltiples métricas de similitud de texto usando el dataset de Quora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explicación breve\n",
    "\n",
    "**Flujo Completo del Código**\n",
    "\n",
    "1. Dataset:\n",
    "Descargamos y cargamos el dataset de Quora, obteniendo las consultas y el corpus.\n",
    "\n",
    "2. Embeddings:\n",
    "Usamos Sentence Transformers para convertir las consultas y documentos en vectores densos.\n",
    "\n",
    "3. Similitud:\n",
    "Calculamos la similitud coseno entre consultas y documentos.\n",
    "Extraemos los documentos más relevantes para cada consulta.\n",
    "\n",
    "4. Resultado:\n",
    "Mostramos los pares de consulta-documento con mayor similitud.\n",
    "\n",
    "**Ejemplo de Salida**\n",
    "\n",
    "***Supongamos que la consulta es:***\n",
    "\n",
    "Consulta: How can I learn Python effectively?\n",
    "\n",
    "***Documentos más similares:***\n",
    "\n",
    "1. DocID: 12345, Puntaje: 0.87, Texto: What are the best resources for learning Python?\n",
    "2. DocID: 67890, Puntaje: 0.82, Texto: How do I start with Python programming?\n",
    "3. DocID: 13579, Puntaje: 0.78, Texto: What are tips to become proficient in Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/nlp_tutorials/lib/python3.10/site-packages/beir/util.py:2: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b5c37a52b504c33b0bcf7d9d4918e15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/522931 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from beir import util\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "\n",
    "# Descarga el dataset Quora\n",
    "dataset_url = \"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/quora.zip\"\n",
    "data_path = util.download_and_unzip(dataset_url, \"datasets/\")  # Descarga y descomprime\n",
    "\n",
    "# Cargar el dataset\n",
    "corpus, queries, qrels = GenericDataLoader(data_path).load(split=\"test\")  # Usa el split \"test\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Qué es Sentence Transformers?**\n",
    "\n",
    "* Sentence Transformers es una biblioteca que utiliza modelos basados en Transformers (como BERT o RoBERTa) para generar embeddings de alta calidad.\n",
    "\n",
    "* Estos embeddings capturan la semántica del texto, lo que permite comparar textos mediante operaciones matemáticas como la Similitud Coseno.\n",
    "\n",
    "¿Por qué usamos all-MiniLM-L6-v2?\n",
    "\n",
    "1. Modelo Ligero:\n",
    "* Es un modelo pequeño y eficiente, diseñado para generar embeddings densos rápidamente.\n",
    "* MiniLM utiliza una arquitectura reducida en comparación con modelos más grandes como BERT o RoBERTa, lo que lo hace más rápido y menos exigente en términos de recursos.\n",
    "\n",
    "2. Versatilidad:\n",
    "* Funciona bien en tareas generales de similitud de texto, como búsqueda de documentos o detección de duplicados.\n",
    "* Está optimizado para encontrar similitudes semánticas entre oraciones o párrafos.\n",
    "\n",
    "3. Calidad vs. Eficiencia:\n",
    "* Aunque es más pequeño que otros modelos, mantiene una excelente relación calidad/eficiencia.\n",
    "* Es ideal para aplicaciones prácticas en tiempo real o con grandes volúmenes de datos.\n",
    "\n",
    "4. Preentrenamiento:\n",
    "* Fue preentrenado específicamente para tareas de comparación de oraciones y recuperación de información.\n",
    "* Es compatible con métricas como Similitud Coseno y Distancia Euclidiana.\n",
    "\n",
    "**¿Qué hacen los embeddings generados por este modelo?**\n",
    "\n",
    "Transforman textos en vectores densos. Por ejemplo, el texto \"¿Cómo aprender Python rápidamente?\" se convierte en un vector numérico.\n",
    "\n",
    "* Los vectores se pueden comparar para medir la similitud semántica entre textos.\n",
    "* Los embeddings permiten realizar búsquedas eficientes, detectar duplicados o clasificar textos relacionados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generados para 50 consultas y 1000 documentos.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Cargar modelo de Sentence Transformers\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Seleccionar 50 consultas\n",
    "subset_queries = list(queries.values())[:50]  # Ajusta el número según lo que necesites\n",
    "subset_corpus = list(corpus.values())[:1000]  # También puedes reducir el corpus\n",
    "\n",
    "# Generar embeddings solo para el subconjunto\n",
    "query_embeddings = model.encode(subset_queries, convert_to_tensor=True)\n",
    "corpus_embeddings = model.encode(subset_corpus, convert_to_tensor=True)\n",
    "\n",
    "print(f\"Embeddings generados para {len(subset_queries)} consultas y {len(subset_corpus)} documentos.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similitud del Coseno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Which question should I ask on Quora?\n",
      "\n",
      "Most Similar Documents:\n",
      "1. Similarity Score: 0.6616\n",
      "   Document Content: {'text': 'What are the questions should not ask on Quora?', 'title': ''}\n",
      "\n",
      "2. Similarity Score: 0.4923\n",
      "   Document Content: {'text': 'How do you answer a question on Quora?', 'title': ''}\n",
      "\n",
      "3. Similarity Score: 0.4099\n",
      "   Document Content: {'text': \"What is the best thing you've ever learned on Quora?\", 'title': ''}\n",
      "\n",
      "4. Similarity Score: 0.4046\n",
      "   Document Content: {'text': 'What should I ask my crush?', 'title': ''}\n",
      "\n",
      "5. Similarity Score: 0.3705\n",
      "   Document Content: {'text': 'Why nobody answer my questions in Quora?', 'title': ''}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch import topk\n",
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "# Calculate cosine similarity between queries and documents\n",
    "similarity_scores_cosine = cos_sim(query_embeddings, corpus_embeddings)\n",
    "\n",
    "# Get the most similar documents for the first query\n",
    "top_k = 5\n",
    "scores_cosine_p1, indices_cosine_p1 = topk(similarity_scores_cosine[0], k=top_k)\n",
    "\n",
    "# Convert tensor to NumPy arrays\n",
    "scores_cosine_p1 = scores_cosine_p1.cpu().numpy()\n",
    "indices_cosine_p1 = indices_cosine_p1.cpu().numpy()\n",
    "\n",
    "# Retrieve the first query's content\n",
    "query_content = list(queries.values())[0]\n",
    "\n",
    "# Retrieve the content of the most similar documents\n",
    "similar_docs = [list(corpus.values())[idx] for idx in indices_cosine_p1]\n",
    "\n",
    "# Display the query and the most similar documents\n",
    "print(f\"Query: {query_content}\\n\")\n",
    "print(\"Most Similar Documents:\")\n",
    "for i, (doc, score) in enumerate(zip(similar_docs, scores_cosine_p1), start=1):\n",
    "    print(f\"{i}. Similarity Score: {score:.4f}\")\n",
    "    print(f\"   Document Content: {doc}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distancia euclidiana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distancias Euclidianas calculadas con éxito.\n",
      "Query: Which question should I ask on Quora?\n",
      "\n",
      "Most Similar Documents (Euclidean Distance):\n",
      "1. Distance: 0.8227\n",
      "   Document Content: {'text': 'What are the questions should not ask on Quora?', 'title': ''}\n",
      "\n",
      "2. Distance: 1.0077\n",
      "   Document Content: {'text': 'How do you answer a question on Quora?', 'title': ''}\n",
      "\n",
      "3. Distance: 1.0863\n",
      "   Document Content: {'text': \"What is the best thing you've ever learned on Quora?\", 'title': ''}\n",
      "\n",
      "4. Distance: 1.0912\n",
      "   Document Content: {'text': 'What should I ask my crush?', 'title': ''}\n",
      "\n",
      "5. Distance: 1.1221\n",
      "   Document Content: {'text': 'Why nobody answer my questions in Quora?', 'title': ''}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch import cdist\n",
    "import numpy as np\n",
    "\n",
    "# Create a subset of embeddings (first 50 queries and first 1000 documents)\n",
    "subset_queries = query_embeddings[:50]  # Adjust as needed\n",
    "subset_corpus = corpus_embeddings[:1000]  # Adjust as needed\n",
    "\n",
    "# Compute Euclidean distances between queries and the corpus\n",
    "distances_euclidean = cdist(subset_queries, subset_corpus, p=2).cpu().numpy()\n",
    "print(\"\\nDistancias Euclidianas calculadas con éxito.\")\n",
    "\n",
    "# Select the 5 closest documents for the first query\n",
    "top_k = 5\n",
    "distances_euclidean_p1 = distances_euclidean[0]\n",
    "indices_euclidean_p1 = np.argsort(distances_euclidean_p1)[:top_k]  # Indices of the smallest distances\n",
    "scores_euclidean_p1 = distances_euclidean_p1[indices_euclidean_p1]\n",
    "\n",
    "# Retrieve the first query's content\n",
    "query_content = list(queries.values())[0]\n",
    "\n",
    "# Retrieve the content of the most similar documents\n",
    "similar_docs = [list(corpus.values())[idx] for idx in indices_euclidean_p1]\n",
    "\n",
    "# Display the query and the most similar documents\n",
    "print(f\"Query: {query_content}\\n\")\n",
    "print(\"Most Similar Documents (Euclidean Distance):\")\n",
    "for i, (doc, score) in enumerate(zip(similar_docs, scores_euclidean_p1), start=1):\n",
    "    print(f\"{i}. Distance: {score:.4f}\")\n",
    "    print(f\"   Document Content: {doc}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similitud de Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la función de distancia de Jaccard\n",
    "def jaccard_distance(text1, text2):\n",
    "    set1, set2 = set(text1.split()), set(text2.split())\n",
    "    intersection = len(set1 & set2)\n",
    "    union = len(set1 | set2)\n",
    "    return 1 - (intersection / union) if union != 0 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Which question should I ask on Quora?\n",
      "\n",
      "Most Similar Documents (Jaccard Distance):\n",
      "1. Distance: 0.6667\n",
      "   Document Content: What are the questions should not ask on Quora?\n",
      "\n",
      "2. Distance: 0.7000\n",
      "   Document Content: What should I ask my crush?\n",
      "\n",
      "3. Distance: 0.7000\n",
      "   Document Content: Can I earn money on Quora?\n",
      "\n",
      "4. Distance: 0.7500\n",
      "   Document Content: How do you answer a question on Quora?\n",
      "\n",
      "5. Distance: 0.7778\n",
      "   Document Content: How should I study\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define Jaccard Distance function\n",
    "def jaccard_distance(text1, text2):\n",
    "    set1, set2 = set(text1.split()), set(text2.split())\n",
    "    intersection = len(set1 & set2)\n",
    "    union = len(set1 | set2)\n",
    "    return 1 - (intersection / union) if union != 0 else 1\n",
    "\n",
    "# Select a subset of data (50 queries and 1000 documents)\n",
    "top_k = 5\n",
    "subset_queries = [q['text'] if isinstance(q, dict) else q for q in list(queries.values())[:50]]\n",
    "subset_corpus = [doc['text'] if isinstance(doc, dict) else doc for doc in list(corpus.values())[:1000]]\n",
    "\n",
    "# Calculate Jaccard distances between the first query and all documents\n",
    "jaccard_distances_question_p1 = [jaccard_distance(subset_queries[0], doc) for doc in subset_corpus]\n",
    "indices_jaccard_p1 = np.argsort(jaccard_distances_question_p1)[:top_k]  # Indices of the smallest distances\n",
    "scores_jaccard_p1 = [jaccard_distances_question_p1[i] for i in indices_jaccard_p1]\n",
    "\n",
    "# Retrieve the content of the most similar documents\n",
    "query_content = subset_queries[0]\n",
    "similar_docs = [subset_corpus[idx] for idx in indices_jaccard_p1]\n",
    "\n",
    "# Display the query and the most similar documents\n",
    "print(f\"Query: {query_content}\\n\")\n",
    "print(\"Most Similar Documents (Jaccard Distance):\")\n",
    "for i, (doc, score) in enumerate(zip(similar_docs, scores_jaccard_p1), start=1):\n",
    "    print(f\"{i}. Distance: {score:.4f}\")\n",
    "    print(f\"   Document Content: {doc}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparamos los 3 resultados\n",
    "\n",
    "* Notamos que la Euclidanea con la de similitud del coseno ordenan de forma similar pero la métrica del coseno captura mejor la similitud al diferenciar más entre preguntas como estas ***What should I ask my crush?*** con ***How do you answer a question on Quora?***\n",
    "\n",
    "* Se muestra que la distancia de jaccard no sirve cuando quieres establecer relaciones semánticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matrix of Document Content and Metric Values:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document Content</th>\n",
       "      <th>Jaccard</th>\n",
       "      <th>Euclidean</th>\n",
       "      <th>Cosine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the questions should not ask on Quora?</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.822656</td>\n",
       "      <td>0.661619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can I earn money on Quora?</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How should I study</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How do you answer a question on Quora?</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.007705</td>\n",
       "      <td>0.492265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Why nobody answer my questions in Quora?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.122088</td>\n",
       "      <td>0.370459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What should I ask my crush?</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.091194</td>\n",
       "      <td>0.404648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is the best thing you've ever learned on ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.086328</td>\n",
       "      <td>0.409946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Document Content   Jaccard  Euclidean  \\\n",
       "0    What are the questions should not ask on Quora?  0.666667   0.822656   \n",
       "1                         Can I earn money on Quora?  0.700000        NaN   \n",
       "2                                 How should I study  0.777778        NaN   \n",
       "3             How do you answer a question on Quora?  0.750000   1.007705   \n",
       "4           Why nobody answer my questions in Quora?       NaN   1.122088   \n",
       "5                        What should I ask my crush?  0.700000   1.091194   \n",
       "6  What is the best thing you've ever learned on ...       NaN   1.086328   \n",
       "\n",
       "     Cosine  \n",
       "0  0.661619  \n",
       "1       NaN  \n",
       "2       NaN  \n",
       "3  0.492265  \n",
       "4  0.370459  \n",
       "5  0.404648  \n",
       "6  0.409946  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the top_k for consistency\n",
    "top_k = 5\n",
    "\n",
    "# Jaccard\n",
    "jaccard_distances_question_p1 = [jaccard_distance(subset_queries[0], doc) for doc in subset_corpus]\n",
    "indices_jaccard_p1 = np.argsort(jaccard_distances_question_p1)[:top_k]\n",
    "scores_jaccard_p1 = [jaccard_distances_question_p1[i] for i in indices_jaccard_p1]\n",
    "\n",
    "# Euclidean\n",
    "distances_euclidean_p1 = distances_euclidean[0]\n",
    "indices_euclidean_p1 = np.argsort(distances_euclidean_p1)[:top_k]\n",
    "scores_euclidean_p1 = [distances_euclidean_p1[i] for i in indices_euclidean_p1]\n",
    "\n",
    "# Cosine\n",
    "cosine_similarities_question_1 = similarity_scores_cosine[0].cpu().numpy()\n",
    "indices_cosine_p1 = np.argsort(-cosine_similarities_question_1)[:top_k]\n",
    "scores_cosine_p1 = [cosine_similarities_question_1[i] for i in indices_cosine_p1]\n",
    "\n",
    "# Consolidate all document indices that appeared in any top 5\n",
    "all_indices = set(indices_jaccard_p1) | set(indices_euclidean_p1) | set(indices_cosine_p1)\n",
    "\n",
    "# Create the DataFrame\n",
    "matrix_data = {\n",
    "    \"Document Content\": [subset_corpus[idx] for idx in all_indices],\n",
    "    \"Jaccard\": [\n",
    "        scores_jaccard_p1[indices_jaccard_p1.tolist().index(idx)] if idx in indices_jaccard_p1 else np.nan\n",
    "        for idx in all_indices\n",
    "    ],\n",
    "    \"Euclidean\": [\n",
    "        scores_euclidean_p1[indices_euclidean_p1.tolist().index(idx)] if idx in indices_euclidean_p1 else np.nan\n",
    "        for idx in all_indices\n",
    "    ],\n",
    "    \"Cosine\": [\n",
    "        scores_cosine_p1[indices_cosine_p1.tolist().index(idx)] if idx in indices_cosine_p1 else np.nan\n",
    "        for idx in all_indices\n",
    "    ],\n",
    "}\n",
    "\n",
    "df_matrix = pd.DataFrame(matrix_data)\n",
    "\n",
    "# Display the matrix\n",
    "print(\"\\nMatrix of Document Content and Metric Values:\")\n",
    "df_matrix\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_tutorials)",
   "language": "python",
   "name": "nlp_tutorials"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
